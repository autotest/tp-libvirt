- virtual_network.qemu_test.performance.netperf: image_copy
    start_vm = no
    virt_test_type = libvirt
    type = netperf
    not_preprocess = yes
    image_snapshot = yes
    setup_ksm = no
    take_regular_screendumps = no
    store_vm_register = no
    kill_vm_libvirt = yes
    # sometimes since vm performance issue or processors could not handle
    # at same time with mis-config sessions number, it may not all clients
    # up at same times, please modify tires parameter to repeat sub tests
    # to increase the successful probalility
    tries = 5
    # Please update following comments params when you need special cfg for
    # your test nic cards
    # nic1 is for control, nic2 is for data connection
    nics += ' nic2'
    # queues = 4
    enable_msix_vectors = yes
    #numa configration
    netperf_with_numa = yes
    # configure netperf test parameters, some seconds will be took to
    # wait all the clients work, this wait time should be less than
    # 0.5 * l, the wait time will augments if you have move
    # threads. So experientially suggest l should be not less than 60.
    l = 60
    #client configuration
    client = localhost
    username_client = root
    #password_client = your_client_password
    shell_client_client = ssh
    shell_port_client = 22
    shell_prompt_client =  \[root@.{0,50}][\#\$]
    #host configuration
    shell_port_host = 22
    username_host = root
    #password_host = your_host_password
    os_type_client = linux
    os_type_host = linux
    shell_prompt_host =  \[root@.{0,50}][\#\$]
    #Test base env configration
    ver_cmd = rpm -q qemu-kvm
    libvirt_ver_cmd = rpm -q libvirt
    netperf_version = 2.7.1
    netperf_pkg = netperf/netperf-2.7.1.tar.bz2
    setup_cmd = "cd /tmp && rm -rf netperf-2.7.1 && tar xvfj netperf-2.7.1.tar.bz2 && cd netperf-2.7.1 && sh autogen.sh && CFLAGS=-Wno-implicit-function-declaration ./configure --enable-burst --enable-demo=yes && make"
    ppc64:
        setup_cmd = "cd /tmp && rm -rf netperf-2.7.1 && tar xvfj netperf-2.7.1.tar.bz2 && cd netperf-2.7.1 && sh autogen.sh && CFLAGS=-Wno-implicit-function-declaration ./configure --build=ppc64 --enable-burst --enable-demo=yes  && make"
    ppc64le:
        setup_cmd = "cd /tmp && rm -rf netperf-2.7.1 && tar xvfj netperf-2.7.1.tar.bz2 && cd netperf-2.7.1 && sh autogen.sh && CFLAGS=-Wno-implicit-function-declaration ./configure --build=ppc64le --enable-burst --enable-demo=yes  && make"
    log_hostinfo_script = scripts/rh_perf_log_hostinfo_script.sh
    host_tuned_profile = "tuned-adm profile virtual-host"
    client_tuned_profile = "tuned-adm profile virtual-host"
    client_kill_linux = "killall netperf"
    client_kill_windows = "taskkill /F /IM netperf*"
    # Now the get status functions are implemented for RHEL and Fedora guests.
    # Not test with other guests, please set this depends on your guest os
    # environment.
    RHEL, Fedora:
        get_status_in_guest = yes
    Linux:
    #    log_guestinfo_script = scripts/rh_perf_log_guestinfo_script.sh
    #    log_guestinfo_exec = bash
    #    log_guestinfo_path = /tmp/log_guestinfo.sh
        server_tuned_profile = "tuned-adm profile virtual-guest"
        server_mtu_cmd = "ip link set dev %s mtu %s"
    Windows:
    #    log_guestinfo_script = scripts/rh_perf_log_guestinfo_script.bat
    #    log_guestinfo_exec = cmd /c
    #    log_guestinfo_path = C:\log_guestinfo.bat
        server_mtu_cmd = "powershell -Command \"Set-NetIPInterface -InterfaceAlias '%s' -AddressFamily IPv4 -NlMtu %s\""
        i386, x86_64:
            cpu_model_flags = ",hv_time,hv_relaxed,hv_vapic,hv_spinlocks=0xfff"
        windows_disable_firewall = "netsh advfirewall set allprofiles state off"
        devcon_dirname = "win7_"
        i386:
            devcon_dirname += "x86"
        x86_64:
            devcon_dirname += "amd64"
    client_mtu_cmd = "ip link set dev %s mtu %s"
    host_mtu_cmd = "ip link set dev %s mtu %s"
    #FIXME: it's a workaround, it needs a better way to disable all the firewall rules
    env_setup_cmd = "systemctl stop firewalld.service ; service iptables stop ; iptables -F ; nft flush ruleset;"
    env_setup_cmd += " echo 2 > /proc/sys/net/ipv4/conf/all/arp_ignore;"
    env_setup_cmd += " echo 0 > /sys/kernel/mm/ksm/run;"
    env_setup_cmd += " echo 0 > /proc/sys/kernel/watchdog;"
    env_setup_cmd += " echo 0 > /proc/sys/kernel/nmi_watchdog;"
    env_setup_cmd += " setenforce 1"
    variants:
        - host_guest:
            Linux:
                no Jeos
                # to test exthost <-> guest:
                # client = <external host's private ip>
                # client_public_ip = <external host's public ip>
                # server_private_ip = <netperf server's private ip>
            Windows:
                netserv_start_cmd = "start /b %s:\netserver-2.6.0.exe"
                guest_ver_cmd = "ver"
                use_cygwin = no
                #client = <external host/guest ip>
                config_cmds = driver_verifier_query
                driver_verifier_query = "verifier /querysettings"
                reboot_after_config = yes
                install_vioprot_cmd = "netcfg -v -l %s -c p -i VIOPROT"
                cdroms += " virtio winutils"
                virtio_win_media_type = iso
                driver_name = netkvm VIOPROT
                devcon_path = "WIN_UTILS:\devcon\${devcon_dirname}\devcon.exe"
                device_name = "Red Hat VirtIO Ethernet Adapter"
                device_hwid = '"PCI\VEN_1AF4&DEV_1000" "PCI\VEN_1AF4&DEV_1041"'
            variants:
                # Throughput tests: TCP_STREAM and TCP_MAERTS with jumbo frames (MTU 9000)
                - throughput:
                    protocols = "TCP_STREAM TCP_MAERTS"
                    mtu = 9000
                    # Use standard sessions and sizes for throughput tests
                    sessions = "1 2 4 8"
                    sizes = "64 256 1024 4096 16384 65535"
                # Latency tests: TCP_RR with standard MTU (1500)
                - latency:
                    protocols = "TCP_RR"
                    mtu = 1500
                    # Use RR-specific sessions and sizes
                    sessions_rr = "1 25 50"
                    sizes_rr = "64 256 4096"
    variants:
        - bridge_test:
            create_vm_libvirt = yes
            master_images_clone = img1
            remove_image_image1 = yes
            ovmf:
                kill_vm_libvirt_options = --nvram
            nic_model_nic1 = virtio
            nic_model_nic2 = virtio
            i386, x86_64:
                nic_model_nic1 = e1000
            netdst_nic1 = switch
            netdst_nic2 = private
            # please fix the mac for nic2 if you needed with this, this can be empty
            # nic_mac_nic2 =
            # please add the physical nic you want to add to your private bridge
            # this can be empty
            # physical_nic =
            # bridge_force_create=yes
            # bridge_nic1 =
        - vhost_vdpa_test:
            dev_type = "vdpa"
            create_vm_libvirt = yes
            remove_image_image1 = no
            driver_queues = 9
            vm_iface_driver = "virtio_net"
            mac_addr = "00:11:22:33:44:00"
            pci_assignable_nic1 = yes
            pci_assignable_nic2 = yes
            iface_dict = {'model': 'e1000', 'type_name': 'bridge', 'source': {'bridge': 'switch'}}
            iface_dict2 = {'model': 'virtio', 'source': {'dev': '/dev/vhost-vdpa-0'}, 'type_name': 'vdpa', 'driver': {'driver_attr': {'queues': '${driver_queues}'}}, 'mac_address': '${mac_addr}'}
